{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction and EDA\n",
    "\n",
    "This notebook extracts the data and performs analysis to test for feature availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Filepaths: ['../data/data/a__geo.csv', '../data/data/a__company.csv']\n",
      "INFO:root:Reading ../data/data/a__geo.csv into key a__geo\n",
      "INFO:root:Reading ../data/data/a__company.csv into key a__company\n",
      "INFO:root:Filepaths: ['../data/data/b__company.csv', '../data/data/b__hierarchy.csv', '../data/data/b__address.csv']\n",
      "INFO:root:Reading ../data/data/b__company.csv into key b__company\n",
      "INFO:root:Reading ../data/data/b__hierarchy.csv into key b__hierarchy\n",
      "INFO:root:Reading ../data/data/b__address.csv into key b__address\n",
      "/Users/cullenpaulisick/opt/anaconda3/envs/webscraping/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3357: DtypeWarning: Columns (11,15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "INFO:root:Merging values of dict_keys(['a__geo', 'a__company'])\n",
      "INFO:root:Merged frames into one with columns Index(['geo_id', 'zipcode_a__geo', 'is_primary_a__geo', 'latitude_a__geo',\n",
      "       'longitude_a__geo', 'elevation_a__geo', 'state_a__geo',\n",
      "       'state_full_name_a__geo', 'area_code_a__geo', 'city_a__geo',\n",
      "       'city_display_a__geo', 'county_a__geo', 'county_fips_a__geo',\n",
      "       'state_fips_a__geo', 'timezone_a__geo', 'daylight_saving_a__geo',\n",
      "       'region_a__geo', 'division_a__geo', 'congress_district_a__geo',\n",
      "       'congress_land_area_a__geo', 'country_a__geo', 'continent_a__geo',\n",
      "       'country_iso2_a__geo', 'vendor_id', 'parent_vendor_id_a__company',\n",
      "       'top_vendor_id_a__company', 'cnt_children_a__company',\n",
      "       'orgtype_id_a__company', 'name_a__company', 'email_a__company',\n",
      "       'phone_a__company', 'fax_a__company', 'dunsnumber_a__company',\n",
      "       'websiteurl_a__company', 'address_a__company', 'address1_a__company',\n",
      "       'address2_a__company', 'country_a__company', 'zipcode_a__company',\n",
      "       'parentdunsnumber_a__company', 'score_a__company', 'cnt_opp_a__company',\n",
      "       'bucket_id_a__company', 'load_date_a__company', 'lvl_a__company'],\n",
      "      dtype='object')\n",
      "INFO:root:Merging values of dict_keys(['b__company', 'b__hierarchy', 'b__address'])\n",
      "INFO:root:Merged frames into one with columns Index(['b_entity_id', 'entity_name_b__company',\n",
      "       'entity_proper_name_b__company', 'primary_sic_code_b__company',\n",
      "       'industry_code_b__company', 'sector_code_b__company',\n",
      "       'iso_country_b__company', 'metro_area_b__company',\n",
      "       'state_province_b__company', 'zip_postal_code_b__company',\n",
      "       'web_site_b__company', 'entity_type_b__company',\n",
      "       'entity_sub_type_b__company', 'year_founded_b__company',\n",
      "       'iso_country_incorp_b__company', 'iso_country_cor_b__company',\n",
      "       'nace_code_b__company', 'b_parent_entity_id_b__hierarchy',\n",
      "       'b_ultimate_parent_entity_id_b__hierarchy', 'address_id_b__address',\n",
      "       'location_city_b__address', 'state_province_b__address',\n",
      "       'location_postal_code_b__address', 'city_state_zip_b__address',\n",
      "       'location_street1_b__address', 'location_street2_b__address',\n",
      "       'location_street3_b__address', 'iso_country_b__address',\n",
      "       'tele_country_b__address', 'tele_area_b__address', 'tele_b__address',\n",
      "       'tele_full_b__address', 'fax_country_b__address', 'fax_area_b__address',\n",
      "       'fax_b__address', 'fax_full_b__address', 'hq_b__address'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from typing import Dict, Union, List\n",
    "import logging\n",
    "from functools import reduce\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# set logger level\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "def data_into_dict(\n",
    "        filepath: Union[str, List], \n",
    ") -> Dict[str, pd.DataFrame]:   \n",
    "    \"\"\"Read data from paths into dictionary values\n",
    "    This is an example of Google style.\n",
    "\n",
    "    Args:\n",
    "        filepath (Union[str, List]) : string literal of list of strings pointing to files for io\n",
    "    Returns:\n",
    "        file_d: dictionary of files as dataframes, with key as filename abbreviation\n",
    "    \"\"\"\n",
    "    logging.info(f\"Filepaths: {filepath}\")\n",
    "    # create file dictionary\n",
    "    file_d = dict()\n",
    "    for f in filepath:\n",
    "        # get abbreviation for key\n",
    "        fname_abbr =  os.path.split(f)[1].split(\".\")[0]\n",
    "        logging.info(f\"Reading {f} into key {fname_abbr}\")\n",
    "        # read dataframe into value\n",
    "        file_d[fname_abbr] = pd.read_csv(f)\n",
    "    return file_d\n",
    "\n",
    "\n",
    "# read in data from dir\n",
    "data_path = \"../data/data/\"\n",
    "# group filenames by prefix\n",
    "a_files = data_into_dict(glob.glob(os.path.join(data_path, \"a__*\")))\n",
    "b_files = data_into_dict(glob.glob(os.path.join(data_path, \"b__*\")))\n",
    "\n",
    "# consolidate dataframe groups into merged structure \n",
    "def merge_all_frames(\n",
    "    frames: Dict, \n",
    "    on: str, \n",
    "    how: str, \n",
    "    rename_exclusions: List=[]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Merge all frames in list into single dataframe \n",
    "\n",
    "    Args:\n",
    "        frames (Dict) : dict of dfs with frame values to merge into single frame\n",
    "        on (str) : column to merge on \n",
    "        how (str) : merge type\n",
    "        rename (List) : list of columns to not rename\n",
    "    Returns:\n",
    "        frame_merged (pd.DataFrame): merged dataframe\n",
    "    \"\"\"\n",
    "    frames = deepcopy(frames)\n",
    "    # append df key names to columns to resolve conflicts in col names\n",
    "    for frame in frames.items(): \n",
    "        frames[frame[0]] = frame[1].rename({col:f\"{col}_{frame[0]}\" \\\n",
    "            for col in frame[1] if (col!=on) and (col not in rename_exclusions)}, axis=1)\n",
    "\n",
    "    # merge frames and set key value as conflicting column suffixes\n",
    "    logging.info(f\"Merging values of {frames.keys()}\")\n",
    "    frame_merged = reduce(\n",
    "                lambda  left,right: pd.merge(left,right,on=[on],how=how), frames.values()\n",
    "            )\n",
    "        \n",
    "    logging.info(f\"Merged frames into one with columns {frame_merged.columns}\")\n",
    "    return frame_merged\n",
    "\n",
    "a_frame = merge_all_frames(a_files, on=\"geo_id\", how='outer', rename_exclusions=['vendor_id'])\n",
    "b_frame = merge_all_frames(b_files, on=\"b_entity_id\", how='outer')\n",
    "\n",
    "a_frame = a_frame.set_index(\"vendor_id\").reset_index()\n",
    "b_frame = b_frame.set_index(\"b_entity_id\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_compression(\n",
    "    frame: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compress types of values into a standard\n",
    "\n",
    "    Args:\n",
    "        frame (pd.DataFrame) : data frame to compress types\n",
    "    Returns: \n",
    "        frame_comp (pd.DataFrame) : compressed type data\n",
    "    \"\"\"\n",
    "    # get column types\n",
    "    col_types = frame.dtypes.to_dict()\n",
    "    # replace na value with string literal\n",
    "    frame = frame.fillna(\"null\")\n",
    "    # convert object types to string values\n",
    "    for col, datatype in col_types.items():\n",
    "        frame[col] = frame[col].astype(str)\n",
    "        # get all values in series that are not null\n",
    "        non_null_s = frame[col][frame[col].apply(lambda x: x!='null')]\n",
    "        isnum_s = non_null_s.apply(lambda x: x.replace(\"-\", \"\").replace(\".\", \"\").isnumeric())\n",
    "        isnum_sum, isnum_true = isnum_s.value_counts().sum(), isnum_s.value_counts().get(True)\n",
    "        # get proportion of numeric strings in non-null series\n",
    "        isnum_prop = isnum_true/isnum_sum\n",
    "        # check proprtion and convert based upon value\n",
    "        if isnum_prop > 0.80:\n",
    "            col_numeric = frame[col].apply(lambda x: x.replace(\"-\", \"\").replace(\".\", \"\").isnumeric())\n",
    "            # check for int vs float values\n",
    "            int_conv = frame[col][col_numeric].apply(lambda x: set(x.split(\".\")[-1]).issubset(\"0\"))\n",
    "\n",
    "            frame.loc[~col_numeric,col] = \"-1\"\n",
    "            frame[col] = frame[col].astype(\"float\")\n",
    "        # if non null values are not primarily numeric, then convert to lowercase\n",
    "        # and remove non-alphanumeric chars\n",
    "        else: \n",
    "\n",
    "                \n",
    "            \n",
    "        \n",
    "       \n",
    "    return frame\n",
    "\n",
    "    \n",
    "# def data_cleaning(\n",
    "#     frame: pd.DataFrame\n",
    "# ) -> pd.DataFrame :\n",
    "#     \"\"\"Clean data from compressed types\n",
    "    \n",
    "#     Args: \n",
    "#         frame (pd.DataFrame) : to be worked dataframe\n",
    "#     Returns: \n",
    "#         frame_clean (pd.DataFrame) : cleaned frame\n",
    "#     \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "comp = type_compression(frame=a_frame[['longitude_a__geo', 'vendor_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         True\n",
       "1         True\n",
       "2         True\n",
       "3         True\n",
       "4         True\n",
       "         ...  \n",
       "76339     True\n",
       "76340     True\n",
       "76341     True\n",
       "76342     True\n",
       "76343    False\n",
       "Name: zipcode_a__company, Length: 76344, dtype: bool"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isnum = a_frame.zipcode_a__company.astype(str).apply(lambda x: x.replace(\"-\", \"\").replace(\".\", \"\").isnumeric())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        False\n",
       "1        False\n",
       "2        False\n",
       "3        False\n",
       "4        False\n",
       "         ...  \n",
       "76339    False\n",
       "76340    False\n",
       "76341    False\n",
       "76342    False\n",
       "76343    False\n",
       "Name: zipcode_a__company, Length: 76344, dtype: bool"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_frame['zipcode_a__company'][isnum].astype(str).apply(lambda x: set(x.split(\".\")[-1]).issubset(\"0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 ('webscraping')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28e85b6ddbb5dc1028087489c79e17921036533c9bfed4bb1ebacd9fa0618340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
